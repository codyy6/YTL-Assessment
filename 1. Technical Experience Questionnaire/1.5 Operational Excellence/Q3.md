# 1.5.3 Share an example (or hypothetical) of how you'd design for scalability in an AI application (e.g., batching, caching, horizontal scaling).

---

- Inference layer is made stateless
- Load Balancing to distribute requests to prevent heavy traffic
- Each instance runs the model with dynamic batching, requests within a short micro-window are grouped into batches to raise GPU utilization
- Frequently used prompts and outputs are cached in Redis, services checks cache before invoking model
- Autoscaling being implemented, and new instances are spun up before queues gets too long.

