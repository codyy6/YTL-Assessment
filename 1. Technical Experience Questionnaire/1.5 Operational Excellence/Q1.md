# 1.5.1 AI systems often require trade-offs between speed, cost, and accuracy. How do you define these requirements and ensure they are met in production?

---

There are different perspectives that I would look at it from: budget, use case requirements, and dataset.

## Budget

Based on my experience, teams usually have been allocated a certain amount of budget annually. Before the development of the AI systems, there should be a discussion on how much budget to be used. Based on the discussed budget, then only the accuracy and speed of the system can be determined. In some cases where either the accuracy or speed of the system cannot be compromised, then the budget needs to be revisited to see how to adjust based on the situation.

## Use Case Requirements

If the use case requires real-time interaction, latency becomes the primary constraint, often requiring a trade-off where a less accurate or more costly model is used. If the use case is a daily report, accuracy can be prioritized, allowing for slower, more complex, and potentially less costly models. For internal or non-critical applications, cost might be the main driver, forcing the team to accept lower accuracy or higher latency by using smaller models or cheaper infrastructure.

## Dataset

Having a large dataset will directly cause the computational cost to increase and latency to take longer. In cases where the accuracy can be sacrificed a little bit while increasing the speed and reducing cost, different techniques can be used such as pruning and quantization.

In cases where the dataset is imbalanced, the constraint for the minority class needs to be defined first by getting its recall value and that is the minimum acceptable recall. Once that is fixed, the model size and training load can be reduced. During the reduction of model size and training load, measure all 3 criteria.
