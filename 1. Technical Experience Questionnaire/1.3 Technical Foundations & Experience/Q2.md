# 1.3.2 Share examples of the frameworks, libraries, or cloud services you've used for AI engineering (e.g., PyTorch, Hugging Face, LangChain, vector DBs, cloud ML services). What factors influenced your choice?

---

I'm currently working with several tools in an AI engineering context, each chosen for specific reasons:

**LLM Services:**

-   **Qwen LLM**: Chosen for on-premise deployment capability, which satisfies regulatory compliance requirements.
-   **OpenAI API**: Alternative option for different companies based on their regulatory compliance or requirement

**Infrastructure:**

-   **PostgreSQL**: Stores structured metadata (user sessions, query logs, feedback) with relational integrity. Uses pgvector extension for hybrid search scenarios where I need to combine vector similarity with SQL filtering.
-   **Redis**: Caches frequently accessed embeddings and API responses, reducing inference latency from ~2s to <200ms for repeated queries. Also handles rate limiting and session management.
-   **RabbitMQ**: Manages asynchronous job queues for batch embedding generation and long-running tasks, decoupling API requests from processing to maintain responsiveness under load.
-   **Pinecone** (evaluating): Considering for dedicated vector search at scale.

**AI/ML Frameworks** (from side projects and learning):

-   **LangChain**: Used for prototyping RAG pipelines, though I've found it adds abstraction overhead for production use cases
