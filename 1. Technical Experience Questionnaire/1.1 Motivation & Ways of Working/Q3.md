# 1.3 How do you usually collaborate with product managers, platform engineers, or data/research teams to deliver AI solutions?

---

**Product Managers**

Mainly liaising about the requirements, acceptance criteria, project timeline, user story, story points (if using agile methodology), user flow, expected behaviour or complex behaviour, UI thought process (if UI design is required by dev)

If faced with any question or concern, rediscuss with product managers about the question or concern and come to a consensus. If the question or concern is tech related, provide suggestions to them as well, so they can make a more confident and suitable decision.

**Platform Engineer**

(never worked with a platform engineer, so this is my assumption based on the job scope of platform engineer i can find online.)

Convert product requirements into clear technical boundaries that guide infrastructure decisions. Define acceptable failure rates, response-time budgets, and other operational metrics, and clarify which logic belongs in the model stack versus the application layer.

Align with platform engineers on API structures, data formats, model outputs, versioning rules, and fine-tuning workflows, and handle failures or performance issues through joint debugging.

After deployment, monitor system health, adjust scaling behavior, maintain security constraints, and update pipelines as the model and requirements evolve.

**Data/Research team**

(I have never worked with a data/research team either, so this is my assumption based on the job scope of data/research team i can find online.)

Define the target behaviors, data boundaries, evaluation standards, and model constraints that translate product goals into concrete research direction. Specify what the model must infer from data and what must be imposed through rules to prevent misalignment.
Establish precise feature definitions, unify interpretation of outputs, and isolate edge cases through systematic analysis of experiments and error distributions. Remove ambiguity by locking down how each category of output should behave under varied conditions.
Validate model performance against real-world drift, audit data quality on a recurring basis, and update evaluation pipelines as signals evolve. Maintain continuity across model revisions to protect interface stability and product-level guarantees.
