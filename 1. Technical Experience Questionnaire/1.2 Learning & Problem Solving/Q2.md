# 1.2.2 Share an example of how you approached a new or unfamiliar AI problem using a systematic, experiment-driven process.

---

One of the example i faced is for my FYP when im trying to build an AI model that capture vulnerabilities in smart contract, and this is the approach:

**Objective and constraints**

The system's objective was to detect a defined list of smart-contract vulnerabilities with a measurable threshold. I set explicit targets for minimum model accuracy, and response-time constraints for inference.

Instead of detecting all different types of vulnerabilities, I set it so that it will capture the few most common vulnerabilities, as there are more data for training. For the model accuracy, I set it so that it would need to achieve an accuracy of above 80%, and the response time is ideally within 3-5 seconds.

**Dataset Search**

I enumerated all publicly available labelled smart-contract datasets and assessed their suitability by volume, annotation quality, Solidity version, and duplication rate. Data quantity is not the only criteria, the distribution coverage and label reliability matters a lot too.

**Dataset Audit**

A full audit is done by finding out the class imbalance, noise patterns, inconsistencies, and Solidity-version drift. Each issue was mapped to a modeling constraint. Skew dictated sampling strategy. Version drift signalled a generalisation ceiling. Noise required preprocessing rules.

**Baseline experiments**

Evaluation showed memorisation of lexical tokens, overfitting, and repeated failures on vulnerabilities tied to control flow or variable-lifetime semantics. After using Cross-dataset testing, it confirmed the poor generalisation.

**Insight Extraction and Pivot**

Failure modes pointed to a representation deficit rather than parameter tuning issues. Vulnerability cues were structural, not lexical. I reformulated the representation requirement and pivoted to Abstract Syntax Treeâ€“based inputs because ASTs expose syntactic hierarchy and data-flow relationships missing from token sequences.

**Reformulated Hypothesis**

AST-derived graph structures paired with graph neural networks would raise generalisation by encoding structural dependencies the NLP models could not model.

**Outcome**

AST-based models produced higher stability and significantly stronger generalisation than the NLP baseline. Final performance was bounded by dataset constraints: labels tied to older Solidity versions and limited coverage of newer language constructs. The model functions as a proof-of-concept demonstrating viability of structural representations but requires retraining on an updated corpus to become production-relevant.

