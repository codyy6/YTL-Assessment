# 1.4.3 What's your approach to handling the following in AI systems?

## a. Technical debt vs. new features

**Low impact technical debt and minor feature**

In such cases, usually i will recommend to not purposely look into the technical debt, but only start looking into it when there are some people that is working on that part of the code. Then at that time they can resolve the technical debt along with their feature.

**High impact technical debt and minor feature**

Will recommend to put seniors to work on the technical debt, especially if it is highly technical, but if it is high impact but not technical intensive, probably can ask a senior and a junior to work on it together so the junior can expand their knowledge while also resolving the tech debt.

**Low impact technical debt and major feature**

If the technical debt is within the major feature, then focus on finishing up the major feature first, if the project is still within timeline, then only proceed to work on the technical debt, else just ignore it till next time when working on the part of the code again

**High impact technical debt and major feature**

This is a bit tricky. The team (or maybe just the seniors) need to discuss the severity of both situations. Then, the skillset of the team members will need to be considered. Are there people who have some certain knowledge or capability that can help tackle specific issues? What will the consequences be if 1 got delayed over the other? How difficult will it be for seniors to take over the work if their work has been done. From there, only then a decision can be made on whether to work on the technical debt or the new feature

## b. Monitoring & observability (what do you monitor at both the system and model level, and why?)

### System Observability

**Resource Utilisation**

Need to ensure that the system is running in good health to prevent any downtime or slow responses caused by lack of resources

**Error Rates**

Observe what is the frequency for errors to happen. What can be the cause of it, any pattern to it. Through this, mitigation plan can be planned to prevent worsening of the error rates.

**Latency**

To ensure good customer satisfaction, by maintaining a low latency, so users can get their responses faster

**Traffic**

Understand when is the peak of traffic, then possible plans of spinning up more kubernetes at peak traffic if it is hindering the performance of the system.

### Model Observability

**Performance metrics**

To ensure the model is still meeting the necessary business goals and hasn't degraded since its initial deployment.

**Prediction drift**

A shift here often indicates that the model is losing confidence or that the underlying concept it's trying to predict is changing (Concept Drift). It is an early warning sign before you even get the ground truth to calculate performance metrics.

## c. Application and model security (e.g., handling sensitive data, ensuring safe use of models)

### Application Security

-   Rate Limiting
-   Model Output Filtering
-   Access Control
-   Input Sanitation
-   Encryption

### Model Security

-   Isolation of model pipeline
-   Handling of PII
-   Adversarial Example
-   Data Poisoning
