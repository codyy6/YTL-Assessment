# 2.2 You need to build a searchable knowledge base chatbot for customer support using an LLM.

Outline, at a high level, how you would:

1. Ingest documents,
2. Store and retrieve them efficiently,
3. Ensure fast and accurate answers to user queries.

---

## 1. Ingest Documents

**Normalize and segment source documents into chunks**

-   Convert documents from various formats (PDF, DOCX, HTML, Markdown) into a standardized text format
-   Segment documents into semantically coherent chunks with appropriate overlap to preserve context
-   Ensure chunk boundaries respect sentence and paragraph boundaries to maintain semantic integrity

**Extract metadata from each document**

-   Extract document-level metadata (title, author, creation date, source URL, document type)
-   Extract chunk-level metadata (chunk index, position in document, section headers, language)
-   Store metadata for filtering and retrieval purposes

**Run batch embedding generation with the same model version**

-   Use a consistent embedding model version across all documents to ensure vector space compatibility
-   Generate embeddings in batches for efficiency
-   Cache embeddings to avoid recomputation

**Validate chunk boundaries with automated overlap checks**

-   Implement automated checks to ensure proper overlap between adjacent chunks
-   Verify that chunk boundaries don't split important semantic units (sentences, paragraphs)
-   Validate that overlap is sufficient to maintain context continuity

## 2. Store and Retrieve Efficiently

**Store chunks and metadata in a vector database with approximate nearest neighbor indexing**

-   Use a vector database (e.g., Pinecone, Weaviate, Qdrant, or Milvus) optimized for similarity search
-   Implement approximate nearest neighbor (ANN) indexing (e.g., HNSW, IVF) for fast retrieval
-   Store embeddings, chunk text, and metadata together for efficient access

**Maintain a parallel lightweight keyword/ID index for exact lookups**

-   Create a separate keyword-based index (e.g., Elasticsearch, or simple inverted index) for exact matches
-   Enable filtering by metadata (document type, date range, source) without vector search
-   Support hybrid search combining vector similarity and keyword matching

**Version embeddings to enable atomic re-indexing without downtime**

-   Implement versioning for embeddings to support model updates
-   Enable atomic swapping between old and new index versions
-   Maintain backward compatibility during transitions

## 3. Ensure Fast and Accurate Answers

**At query time, embed the user query, retrieve top-k chunks via vector search plus keyword precision filters**

-   Embed the user query using the same embedding model
-   Perform vector similarity search to retrieve top-k most relevant chunks
-   Apply keyword filters to narrow down results based on specific terms or metadata
-   Combine vector search and keyword matching for hybrid retrieval

**Rank with a reranker**

-   Use a cross-encoder reranker model to score and reorder retrieved chunks
-   Improve precision by focusing on the most relevant context
-   Reduce noise from less relevant chunks

**Feed only the highest-scoring context into a constrained LLM answer template**

-   Select top-ranked chunks after reranking
-   Construct a constrained prompt template with retrieved context
-   Limit context to fit within the LLM's context window

**Enforce strict context windows, deduplication, and evidence gating to prevent hallucination**

-   Strictly enforce context window limits to prevent truncation issues
-   Deduplicate overlapping chunks to avoid redundant information
-   Implement evidence gating: only allow answers that can be directly supported by retrieved context
-   Add citations linking answers back to source documents
-   Reject queries where insufficient relevant context is found
