# 2.2 You need to build a searchable knowledge base chatbot for customer support using an LLM.

Outline, at a high level, how you would:

1. Ingest documents,
2. Store and retrieve them efficiently,
3. Ensure fast and accurate answers to user queries.

---

## 1. Ingest Documents

**Normalize and segment source documents into chunks**

-   Convert documents from various formats (PDF, DOCX, HTML, Markdown) into a standardized text format
-   Segment documents into semantically coherent chunks with appropriate overlap to preserve context
-   Ensure chunk boundaries respect sentence and paragraph boundaries to maintain semantic integrity

**Extract metadata from each document**

-   Extract document-level metadata (title, author, creation date, source URL, document type)
-   Extract chunk-level metadata (chunk index, position in document, section headers, language)
-   Store metadata for filtering and retrieval purposes

**Run batch embedding generation with the same model version**

-   Use a consistent embedding model version across all documents to ensure vector space compatibility
-   Generate embeddings in batches for efficiency
-   Cache embeddings to avoid recomputation

## 2. Store and Retrieve Efficiently

**Store chunks and metadata in a vector database with approximate nearest neighbor indexing**

-   Use a vector database (e.g., Pinecone, Weaviate, Qdrant, or Milvus) optimized for similarity search
-   Implement approximate nearest neighbor (ANN) indexing (e.g., HNSW, IVF) for fast retrieval
-   Store embeddings, chunk text, and metadata together for efficient access

**Maintain a parallel lightweight keyword/ID index for exact lookups**

-   Enable filtering by metadata (document type, date range, source) without vector search
-   Support hybrid search combining vector similarity and keyword matching

**Version embeddings to enable re-indexing without downtime**

-   Implement versioning for embeddings to support model updates
-   Enable swapping between old and new index versions
-   Maintain backward compatibility during transitions

## 3. Ensure Fast and Accurate Answers

**At query time, embed the user query, retrieve top-k chunks via vector search plus keyword precision filters**

-   Embed the user query using the same embedding model
-   Perform vector similarity search to retrieve top-k most relevant chunks
-   Apply keyword filters to narrow down results based on specific terms or metadata
-   Combine vector search and keyword matching for hybrid retrieval

**Rank with a reranker**

-   Use a cross-encoder reranker model to score and reorder retrieved chunks
-   Improve precision by focusing on the most relevant context
-   Reduce noise from less relevant chunks

**Feed only the highest-scoring context into a constrained LLM answer template**

-   Select top-ranked chunks after reranking
-   Construct a constrained prompt template with retrieved context
-   Limit context to fit within the LLM's context window

**Enforce strict context windows, deduplication, and evidence gating to prevent hallucination**

-   Strictly enforce context window limits to prevent truncation issues
-   Deduplicate overlapping chunks to avoid redundant information
-   Implement evidence gating: only allow answers that can be directly supported by retrieved context
-   Add citations linking answers back to source documents
-   Reject queries where insufficient relevant context is found

# Frontend Planning

## User Interface

-   Display content through streaming to reduce latency from responses
-   Provide autocomplete feature for more commonly used queries
-   Loading state when waiting for Model's response, or if its thinking model, show the thought process of the thinking
-   If documents or websites are provided, provide the URL to navigate to the resources original source

## History

-   Store user query and model response in database, allow users to change conversation which creates a new session id. Then based on the session id, can feed the text back to LLM as a memory

## Feedbacks & Analytics

-   Calculate latency in the system by calculating the response time
-   Provide window for users to give feedback (by stars, choose responses, thumbs up or down)

## Performance

-   Allow lazy loading for conversation. If the conversation if very long, allow for pagination and infinite scrolling so that when user scroll up the conversation, only it will fetch older messages.
