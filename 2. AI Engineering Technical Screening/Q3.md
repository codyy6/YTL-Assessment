# 2.3 You are tasked to build an application that can summarize long documents in both English and Malay.

How would you approach this problem? In your answer, cover:

1. Preprocessing steps you'd consider,
2. How you'd handle multilingual support (including possible code-switching),
3. The approach for summarization (extractive vs abstractive, model choice, or LLM usage),
4. How you would evaluate the quality of the summaries.

---

## Goal: Set Success Metrics

-   **Compression targets**: Define target compression ratios (e.g., 10:1, 20:1) based on document type
-   **Fidelity**: Ensure summaries accurately represent the original content
-   **Allowed hallucination**: Set thresholds for acceptable factual errors (ideally zero)
-   **Language balance**: Ensure summaries maintain appropriate language distribution for code-switched content
-   **Latency**: Define acceptable response times for synchronous vs asynchronous processing

## 1. Preprocessing Steps

**Ingest: Collect documents in various formats, normalize encoding, remove boilerplate, preserve structural metadata, and log source information**

-   Support multiple input formats (PDF, DOCX, HTML, Markdown, plain text)
-   Normalize text encoding (UTF-8) to handle special characters
-   Remove headers, footers, navigation elements, and other boilerplate content
-   Preserve document structure (headings, sections, lists) as metadata
-   Log source information for traceability and citation

**Preprocessing: Detect language at segment level, clean text, segment sentences, tokenize per language, handle entities and redact PII**

-   Perform language detection at sentence or paragraph level (not just document level)
-   Clean text: remove extra whitespace, normalize punctuation, handle special characters
-   Segment text into sentences respecting language-specific rules
-   Tokenize according to language-specific tokenizers (English vs Malay)
-   Identify and handle named entities appropriately
-   Redact personally identifiable information (PII) before processing

**Chunking: Split documents into semantically coherent chunks with overlap, compute multilingual embeddings, and store metadata like language and source**

-   Split long documents into semantically coherent chunks
-   Add overlap between chunks to preserve context
-   Compute multilingual embeddings for each chunk
-   Store metadata including detected language, source document, chunk position

## 2. Multilingual Support (Including Code-Switching)

**Detect code-switching**

-   Identify segments where languages switch within sentences or paragraphs
-   Use language detection models that can handle mixed-language content
-   Tag segments with language labels for proper processing

**Preserve language tags**

-   Maintain language information throughout the pipeline
-   Store language tags with chunks and sentences
-   Use language tags to guide summarization and generation

**Use multilingual embeddings for retrieval**

-   Employ multilingual embedding models (e.g., multilingual BERT, multilingual sentence transformers)
-   Ensure embeddings capture semantic meaning across both languages
-   Support cross-language similarity search if needed

**Control output language for generation or translation**

-   Allow users to specify target language for summaries
-   Support generating summaries in English, Malay, or maintaining original language distribution
-   Use language-specific generation models or translation when needed

## 3. Approach for Summarization

**Summarization pipeline: Use hierarchical approachâ€”extractive stage to select important content, followed by abstractive summarization using modular components**

-   Implement a two-stage pipeline: extractive followed by abstractive
-   Extractive stage reduces document size by selecting key content
-   Abstractive stage generates fluent summaries from selected content
-   Design as modular components for flexibility and maintainability

**Extractive stage: Score sentences/chunks using semantic similarity, importance, novelty, and redundancy removal to reduce hallucination**

-   Score sentences/chunks based on:
    -   Semantic similarity to document theme
    -   Importance (position, frequency, centrality)
    -   Novelty (avoid redundant information)
    -   Redundancy removal to prevent repetition
-   Select top-scoring content to pass to abstractive stage
-   Reduce hallucination risk by grounding in extracted content

**Abstractive stage: Use multilingual LLMs or fine-tuned seq2seq models, apply hierarchical or retrieval-augmented generation for long documents, and enforce constraints on language, length, and factual grounding**

-   Use multilingual LLMs (e.g., multilingual T5, mBART) or fine-tuned sequence-to-sequence models
-   For very long documents, apply hierarchical summarization (summarize chunks, then summarize summaries)
-   Consider retrieval-augmented generation (RAG) for additional context
-   Enforce constraints:
    -   Language: ensure output matches requested language
    -   Length: enforce compression targets
    -   Factual grounding: only include information present in source

**Prompting & control: Use templated prompts with language, compression, style, and citation instructions; few-shot examples for English, Malay, and code-switched cases**

-   Design templated prompts with clear instructions:
    -   Target language specification
    -   Compression ratio or length limits
    -   Style guidelines (formal, informal, technical)
    -   Citation requirements
-   Provide few-shot examples for:
    -   English summaries
    -   Malay summaries
    -   Code-switched content handling

**Post-processing: Add citations mapping sentences to source chunks, consistency checks, duplicate removal, and language-specific cleanup**

-   Add citations linking summary sentences to source document chunks
-   Perform consistency checks (factual accuracy, language consistency)
-   Remove duplicate information
-   Apply language-specific cleanup (spelling, grammar, formatting)

## 4. Deployment

**Provide synchronous/asynchronous summaries**

-   Support synchronous API for short documents with low latency requirements
-   Support asynchronous processing for long documents with job queues
-   Return job IDs for tracking asynchronous requests

**Cache embeddings**

-   Cache document embeddings to avoid recomputation
-   Cache intermediate results (extracted sentences, chunk summaries)

**Scale inference with GPU/CPU strategies**

-   Use GPU acceleration for LLM inference
-   Implement CPU fallback for cost optimization
-   Scale horizontally based on load

**Expose language and length parameters via API**

-   API parameters:
    -   Target language (English, Malay, or preserve original)
    -   Summary length (word count, compression ratio)
    -   Style preferences
    -   Citation requirements

## 5. Evaluation

**Automatic metrics like ROUGE and BERTScore**

-   **ROUGE**: Measure n-gram overlap between generated and reference summaries
    -   ROUGE-1 (unigram overlap)
    -   ROUGE-2 (bigram overlap)
    -   ROUGE-L (longest common subsequence)
-   **BERTScore**: Semantic similarity using contextual embeddings
-   Calculate metrics separately for English and Malay summaries

**Faithfulness checks**

-   Verify that summary claims are supported by source document
-   Detect and flag hallucinations
-   Measure factual accuracy

**Bilingual human evaluation**

-   Human evaluators fluent in both languages assess:
    -   Summary quality and coherence
    -   Language accuracy and naturalness
    -   Factual correctness
    -   Coverage of important information
-   Use Likert scales or pairwise comparisons

**Regression tests for diverse document types**

-   Create test suite with diverse document types:
    -   Technical documents
    -   News articles
    -   Academic papers
    -   Mixed-language documents
-   Track performance metrics over time
-   Detect regressions when model or pipeline changes
